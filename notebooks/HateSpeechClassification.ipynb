{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HateSpeechClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPr9b0l8xC7KnZZSbYVTvbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kopalgarg/hate_speech_classification/blob/main/notebooks/HateSpeechClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwpn0hxK0oWE"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "import re\n",
        "import torch\n",
        "\n",
        "# -- Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# -- Tweet Preprocessing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "!pip install emot\n",
        "import emot\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from emot.emo_unicode import UNICODE_EMOJI\n",
        "\n",
        "# -- Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# -- Performance Metrics\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve\n",
        "\n",
        "# -- Explainability\n",
        "!pip install shap\n",
        "import shap\n",
        "!pip install lime\n",
        "import lime"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8hxuIEc1Rl7",
        "outputId": "ac0e7ddd-8ee5-473c-e7ba-5f5b284fd031"
      },
      "source": [
        "# Connect G-Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64jbQnuf2Xnb"
      },
      "source": [
        "# Set Data Paths\n",
        "ROOT = \"gdrive/MyDrive/CSC2612/\"\n",
        "general = \"gdrive/MyDrive/CSC2612/data/general\"\n",
        "antiAsian = \"gdrive/MyDrive/CSC2612/data/antiAsian\"\n",
        "prof = \"gdrive/MyDrive/CSC2612/data/prof\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "id13W-Pa2mJd",
        "outputId": "194b5ea6-bb9c-4362-d82a-0beea23e80e8"
      },
      "source": [
        "# Read in General Tweetes\n",
        "generalTweets = pd.read_csv(os.path.join(general,'train_E6oV3lV.csv'))\n",
        "generalTweets = generalTweets[['tweet', 'label']]\n",
        "generalTweets.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  label\n",
              "0   @user when a father is dysfunctional and is s...      0\n",
              "1  @user @user thanks for #lyft credit i can't us...      0\n",
              "2                                bihday your majesty      0\n",
              "3  #model   i love u take with u all the time in ...      0\n",
              "4             factsguide: society now    #motivation      0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd5Fqk5i22pp",
        "outputId": "8b12fafd-1e0b-4b35-c8fb-8d81f076a05f"
      },
      "source": [
        "# Label Key with Examples\n",
        "print('0: neutral')\n",
        "print(generalTweets[generalTweets['label']==0].iloc[1])\n",
        "\n",
        "print('1: hate speech')\n",
        "print(generalTweets[generalTweets['label']==1].iloc[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: neutral\n",
            "tweet    @user @user thanks for #lyft credit i can't us...\n",
            "label                                                    0\n",
            "Name: 1, dtype: object\n",
            "1: hate speech\n",
            "tweet    no comment!  in #australia   #opkillingbay #se...\n",
            "label                                                    1\n",
            "Name: 14, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "6Ny60ZWRpfQe",
        "outputId": "9f8da6bb-9477-444d-83df-3ccfb46207f0"
      },
      "source": [
        "# Read in Prof's Tweets\n",
        "profTweets = pd.read_csv(os.path.join(prof,'B_volunteer_labelled_data_20210913.csv'))\n",
        "profTweets.head(10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet id</th>\n",
              "      <th>clean text of tweet</th>\n",
              "      <th>Does this tweet contain covid-related stigmatizing language against the people of Asian-descent?</th>\n",
              "      <th>If you identify the tweet talking about one or more of the topics below, please check them below (multiple answer possible)</th>\n",
              "      <th>If you have a comment, or want to add another related topic that is not listed in task 2, you can mention it here. Also, if the tweet is a duplicate, put \"duplicate\" here.</th>\n",
              "      <th>Labeller ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://twitter.com/edent/status/1244240695029...</td>\n",
              "      <td>I miss my #atlutd match day family... #5Stripe...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://twitter.com/edent/status/1244379980634...</td>\n",
              "      <td>This happened to ME and MY FAMILY. We were set...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://twitter.com/edent/status/1246427051084...</td>\n",
              "      <td>#Coronavirus..Ban wildlife trade..China</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://twitter.com/edent/status/1239157574441...</td>\n",
              "      <td>I bet most of the cases happened because peopl...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://twitter.com/edent/status/1238240240235...</td>\n",
              "      <td>At the start of this decade we all said that w...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>https://twitter.com/edent/status/1242885979389...</td>\n",
              "      <td>Last year, the Polish FM stated that #Poland w...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>News</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>https://twitter.com/edent/status/1240412108463...</td>\n",
              "      <td>Why are  allowing people to sell necessities a...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>https://twitter.com/edent/status/1245624359344...</td>\n",
              "      <td>Recall... \\n We hope you delete this tweet. URL</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>https://twitter.com/edent/status/1249123462171...</td>\n",
              "      <td>All the foolish talk of reopening things is ju...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>https://twitter.com/edent/status/1238542266928...</td>\n",
              "      <td>people can be such scared sheep😱. IMO panic un...</td>\n",
              "      <td>not stigmatizing</td>\n",
              "      <td>Other</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            tweet id  ... Labeller ID\n",
              "0  https://twitter.com/edent/status/1244240695029...  ...          17\n",
              "1  https://twitter.com/edent/status/1244379980634...  ...          17\n",
              "2  https://twitter.com/edent/status/1246427051084...  ...          17\n",
              "3  https://twitter.com/edent/status/1239157574441...  ...          17\n",
              "4  https://twitter.com/edent/status/1238240240235...  ...          17\n",
              "5  https://twitter.com/edent/status/1242885979389...  ...          17\n",
              "6  https://twitter.com/edent/status/1240412108463...  ...          17\n",
              "7  https://twitter.com/edent/status/1245624359344...  ...          17\n",
              "8  https://twitter.com/edent/status/1249123462171...  ...          17\n",
              "9  https://twitter.com/edent/status/1238542266928...  ...          17\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpQV9DWS5BS-"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "# -- remove URL\n",
        "def remove_url(tweet):\n",
        "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  return url_pattern.sub(r'', tweet)\n",
        "\n",
        "# -- remove HTML\n",
        "def remove_html(tweet):\n",
        "  return BeautifulSoup(tweet, 'lxml').text\n",
        "\n",
        "# -- lowercase\n",
        "def lower_case(tweet):\n",
        "  return tweet.str.lower()\n",
        "\n",
        "# -- covert emojis and emoticons to words\n",
        "def convert_emoji(tweet):\n",
        "  for emot in UNICODE_EMOJI:\n",
        "    tweet = tweet.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "  return tweet\n",
        "\n",
        "# -- remove special characters and non-ASCII characters\n",
        "def remove_special_char(tweet):\n",
        "    tweet = re.sub('@[^\\s]+','',tweet)\n",
        "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
        "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
        "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"â²\", \"\", tweet)\n",
        "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
        "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
        "    tweet = re.sub(r\"â¹\", \"\", tweet)\n",
        "    tweet = re.sub(r\"â½\", \"\", tweet)\n",
        "    tweet = re.sub(r\"â¾\", \"\", tweet)\n",
        "    tweet = re.sub(r\"ã¼berweist\", \"\", tweet)\n",
        "    tweet = re.sub(r\"ã¼cretsiz\", \"\", tweet)\n",
        "    tweet = re.sub(r\"zã¼rich\", \"\", tweet)\n",
        "    tweet = re.sub(r\"ã¼retime\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
        "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
        "    tweet = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'mentioned', tweet)\n",
        "    tweet = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'referance', tweet)\n",
        "    tweet = re.sub(r'£|\\$', 'money', tweet)\n",
        "    tweet = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', ' ', tweet)\n",
        "    tweet = re.sub(r'\\d+(\\.\\d+)?', ' ', tweet) \n",
        "    tweet = re.sub(r'[^\\w\\d\\s]', ' ', tweet)\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "    tweet = re.sub(r'^\\s+|\\s+?$', '', tweet.lower())\n",
        "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
        "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
        "    tweet = re.sub(r\"&amp;\", \"&\", tweet) \n",
        "    tweet = re.sub(r\"_\", \"  \", tweet)\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
        "    for p in punctuations:\n",
        "        tweet = tweet.replace(p, f' {p} ')\n",
        "    return str(tweet)\n",
        "\n",
        "# -- spellcheck\n",
        "def spellcheck(tweet):\n",
        "  spell = SpellChecker(distance = 1, language='en')\n",
        "  words = set(nltk.corpus.words.words())\n",
        "  corrected_tweet= []\n",
        "  misspelled_words = spell.unknown(tweet.split())\n",
        "  for word in tweet.split():\n",
        "    if word in misspelled_words:\n",
        "      corrected_tweet.append(spell.correction(word))\n",
        "    else:\n",
        "      corrected_tweet.append(word)\n",
        "  return \" \".join(corrected_tweet)\n",
        "\n",
        "# -- ensure English\n",
        "def ensure_english(tweet):\n",
        "  words = set(nltk.corpus.words.words())\n",
        "  return \" \".join(w for w in nltk.wordpunct_tokenize(tweet)\\\n",
        "                  if w.lower() in words or not w.isalpha())\n",
        "\n",
        "# -- remove punctuation\n",
        "def remove_punctuation(tweet):\n",
        "  regular_punct = list(string.punctuation)\n",
        "  for punctuation in regular_punct:\n",
        "    if punctuation in tweet:\n",
        "      tweet = tweet.replace(punctuation, ' ')\n",
        "  return tweet.strip()\n",
        "  \n",
        "# -- remove stopwords\n",
        "def remove_stopwords(tweet):\n",
        "  en_stop =set(stopwords.words('english'))\n",
        "  tweet = tweet.split()\n",
        "  tweet = \" \".join([word for word in tweet if not word in en_stop])\n",
        "  return tweet\n",
        "\n",
        "# -- tokenize\n",
        "def tokenize(tweet):\n",
        "  return word_tokenize(tweet)\n",
        "\n",
        "# -- lematize\n",
        "def lematize(tweet):\n",
        "  lem = WordNetLemmatizer()\n",
        "  return [lem.lemmatize(w) for w in tweet]\n",
        "\n",
        "# -- finally, combine words\n",
        "def combine_words(tweet):\n",
        "  return ' '.join(tweet)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxaCfYZZ7OmD"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "\n",
        "  tweet = remove_url(tweet)\n",
        "  tweet = remove_html(tweet)\n",
        "  tweet = convert_emoji(tweet)\n",
        "  tweet = remove_special_char(tweet)\n",
        "  tweet = spellcheck(tweet)\n",
        "  #tweet = ensure_english(tweet)\n",
        "  tweet = remove_punctuation(tweet)\n",
        "  tweet = remove_stopwords(tweet)\n",
        "  tweet = tokenize(tweet)\n",
        "  tweet = lematize(tweet)\n",
        "  tweet = combine_words(tweet)\n",
        "\n",
        "  return tweet\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPhXQrHq-gXx"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(generalTweets['tweet'], generalTweets['label'], test_size=0.99, random_state=42)\n",
        "\n",
        "X_train = X_train.apply(clean_tweet)\n",
        "#X_test = X_test.apply(clean_tweet)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr5Svnj77P8U"
      },
      "source": [
        "# Vectorize\n",
        "\n",
        "# -- Bag of Words (unigrams)\n",
        "cv_unigrams = CountVectorizer(ngram_range = (1,1))\n",
        "X_train_bow = cv_unigrams.fit_transform(X_train)\n",
        "\n",
        "\n",
        "# -- Bag of Words (bigrams)\n",
        "cv_bigrams = CountVectorizer(ngram_range = (2,2))\n",
        "X_train_bbow = cv_bigrams.fit_transform(X_train)\n",
        "\n",
        "\n",
        "# -- TF-IDF\n",
        "vec_tfidf = TfidfVectorizer(min_df = 2, max_df = 0.8, use_idf = True, ngram_range=(1, 1))\n",
        "vec_tfidf.fit(X_train)\n",
        "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDsaNerjA1n_"
      },
      "source": [
        "# Train Baseline Models\n",
        "# NB\n",
        "def naive_bayes_model(feature_vector_x, feature_vector_y):\n",
        "  alpha = [1e-10, 1e-5, 0.1, 1.0, 2.0, 5.0, 10.0]\n",
        "  best_alpha = -1\n",
        "  max_score = 0\n",
        "  for a in alpha:\n",
        "    mnb = MultinomialNB(alpha = a)\n",
        "    scores = sklearn.model_selection.cross_val_score(mnb, feature_vector_x, feature_vector_y, cv = 5)\n",
        "    if np.mean(scores)> max_score:\n",
        "      best_alpha = a\n",
        "      max_score = np.mean(scores)\n",
        "    \n",
        "    print('alpha =', a)\n",
        "    print(np.mean(scores))\n",
        "    print('\\n')\n",
        "  \n",
        "  print('best alpha:', best_alpha)\n",
        "  mnb = MultinomialNB(alpha = best_alpha)\n",
        "  mnb.fit(feature_vector_x, feature_vector_y)\n",
        "  print('train score:', mnb.score(feature_vector_x, feature_vector_y))\n",
        "  return mnb\n",
        "\n",
        "# -- NB with BOW unigram\n",
        "mnb_bow = naive_bayes_model(X_train_bow, y_train)\n",
        "# -- NB with BOW bigram\n",
        "mnb_bbow = naive_bayes_model(X_train_bbow, y_train)\n",
        "# -- NB with TF-IDF\n",
        "mnb_tfidf = naive_bayes_model(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS4a7dOxSlra",
        "outputId": "ca0dc2c5-c3d2-4f41-faba-8cf045940092"
      },
      "source": [
        "# DT\n",
        "def dt_model(feature_vector_x, feature_vector_y):\n",
        "  dtclassifier = DecisionTreeClassifier(criterion='entropy', max_depth=None)\n",
        "  scores = cross_val_score(dtclassifier, feature_vector_x, feature_vector_y, cv = 10)\n",
        "  dtclassifier.fit(feature_vector_x, feature_vector_y)\n",
        "  print('train score:', accuracy_score(dtclassifier.predict(feature_vector_x), feature_vector_y))\n",
        "  return dtclassifier\n",
        "\n",
        "# -- DT with BOW unigram\n",
        "dt_bow = dt_model(X_train_bow, y_train)\n",
        "\n",
        "# -- DT with BOW bigram\n",
        "dt_bbow = dt_model(X_train_bbow, y_train)\n",
        "\n",
        "# -- DT with TF-IDF\n",
        "dt_tfidf = dt_model(X_train_tfidf, y_train)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score: 1.0\n",
            "train score: 1.0\n",
            "train score: 0.9937304075235109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KX6Mpe207p3",
        "outputId": "18930ba8-93ed-416a-f2d5-9904738810fd"
      },
      "source": [
        "# RF\n",
        "def rf_model(feature_vector_x, feature_vector_y):\n",
        "  rfclassifier = DecisionTreeClassifier(criterion='entropy', max_depth=None)\n",
        "  scores = cross_val_score(rfclassifier, feature_vector_x, feature_vector_y, cv = 10)\n",
        "  rfclassifier.fit(feature_vector_x, feature_vector_y)\n",
        "  print('train score:', accuracy_score(rfclassifier.predict(feature_vector_x), feature_vector_y))\n",
        "  return rfclassifier\n",
        "\n",
        "# -- RF with BOW unigram\n",
        "rf_bow = dt_model(X_train_bow, y_train)\n",
        "\n",
        "# -- RF with BOW bigram\n",
        "rf_bbow = dt_model(X_train_bbow, y_train)\n",
        "\n",
        "# -- RF with TF-IDF\n",
        "rf_tfidf = dt_model(X_train_tfidf, y_train)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score: 1.0\n",
            "train score: 1.0\n",
            "train score: 0.9937304075235109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FPSol8vSrS-"
      },
      "source": [
        "# LR\n",
        "def lr_model(feature_vector_x, feature_vector_y):\n",
        "  C_values = [0.001,0.01, 0.1,1,10,100]\n",
        "  best_c = -1\n",
        "  max_score = 0\n",
        "  for c in C_values:\n",
        "    lr = LogisticRegression(C = c, random_state=0, solver = 'lbfgs', multi_class='multinomial')\n",
        "    lr.fit(feature_vector_x, feature_vector_y)\n",
        "    scores = sklearn.model_selection.cross_val_score(lr, feature_vector_x, feature_vector_y, cv = 5)\n",
        "    if np.mean(scores)> max_score:\n",
        "      best_c = c\n",
        "      max_score = np.mean(scores)\n",
        "    \n",
        "    print('c =', c)\n",
        "    print(np.mean(scores))\n",
        "    print('\\n')\n",
        "\n",
        "  lr = LogisticRegression(solver = 'lbfgs', multi_class='multinomial', C=c)\n",
        "  lr.fit(feature_vector_x, feature_vector_y)\n",
        "  print('train score:', accuracy_score(lr.predict(feature_vector_x), feature_vector_y))\n",
        "  return lr\n",
        "\n",
        "# -- LR with BOW unigram\n",
        "lr_bow = lr_model(X_train_bow, y_train)\n",
        "\n",
        "# -- LR with BOW bigram\n",
        "lr_bbow = lr_model(X_train_bbow, y_train)\n",
        "\n",
        "# -- LR with TF-IDF\n",
        "lr_tfidf = lr_model(X_train_tfidf, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1GhEfKhbojl",
        "outputId": "ddaebb94-b810-4dbf-820e-d3ae82a0267a"
      },
      "source": [
        "# SVM\n",
        "def svm_model(feature_vector_x, feature_vector_y):\n",
        "  params = {'C':[0.01, 0.1, 1, 10, 100],\n",
        "       'kernel':['rbf', 'poly', 'linear', 'sigmoid']}\n",
        "  classifier_linear = GridSearchCV(svm.SVC(), params, cv=10)\n",
        "  classifier_linear.fit(feature_vector_x, feature_vector_y)\n",
        "  print('train score:', accuracy_score(classifier_linear.predict(feature_vector_x), feature_vector_y))\n",
        "\n",
        "# -- SVM with BOW unigram\n",
        "svm_bow = svm_model(X_train_bow, y_train)\n",
        "# -- SVM with BOW bigram\n",
        "svm_bbow = svm_model(X_train_bbow, y_train)\n",
        "# -- SVM with TF-IDF\n",
        "svm_tfidf = svm_model(X_train_tfidf, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train score: 1.0\n",
            "train score: 0.9373040752351097\n",
            "train score: 0.9937304075235109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mQYq__mgAzb",
        "outputId": "888c4487-cc53-427f-f883-4c6045a02a9e"
      },
      "source": [
        "# XGBoost\n",
        "def xgboost_model(feature_vector_x, feature_vector_y):\n",
        "  xgb = XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7)\n",
        "  param_grid = {\n",
        "     'xgb__n_estimators': [1, 5, 10, 50, 100, 150, 300]}\n",
        "  grid_search = GridSearchCV(estimator = xgb, param_grid = param_grid, cv = 10, \n",
        "                             n_jobs = 1, verbose = 0, return_train_score=True)\n",
        "\n",
        "  grid_search.fit(feature_vector_x, feature_vector_y)\n",
        "  print(grid_search.best_params_)\n",
        "\n",
        "  print('train score:', accuracy_score(grid_search.predict(feature_vector_x), feature_vector_y))\n",
        "\n",
        "  return xgb\n",
        "\n",
        "# -- XGBoost with BOW unigram\n",
        "xgb_bow = xgboost_model(X_train_bow, y_train)\n",
        "# -- XGBoost with BOW bigram\n",
        "xgb_bbow = xgboost_model(X_train_bbow, y_train)\n",
        "# -- XGBoost with TF-IDF\n",
        "xgb_tfidf = xgboost_model(X_train_tfidf, y_train)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'xgb__n_estimators': 1}\n",
            "train score: 0.9373040752351097\n",
            "{'xgb__n_estimators': 1}\n",
            "train score: 0.9373040752351097\n",
            "{'xgb__n_estimators': 1}\n",
            "train score: 0.9373040752351097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyesEOnDgDJi"
      },
      "source": [
        "# 1D CNN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE8C2IUd0rzS"
      },
      "source": [
        "# RNN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUAJzMx0fwh"
      },
      "source": [
        "# BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwOcg50rBjU_"
      },
      "source": [
        "# Explainability \n",
        "\n",
        "# -- SHAP\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xV6u0T_Su3C"
      },
      "source": [
        "# -- LIME"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}